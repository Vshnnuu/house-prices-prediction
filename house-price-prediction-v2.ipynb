{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Sale Prices Using Advanced Regression and Feature Engineering\n\nIn this notebook I have explored the House Prices dataset from Kaggle using several regression algorithms.\nWe‚Äôll go through the complete machine learning workflow step by step:\n- üßπ Data exploration & cleaning\n\n- ‚öôÔ∏è Feature engineering\n\n- üß† Model building & tuning\n\n- ü§ù Model blending (XGBoost, LightGBM, Ridge, Lasso)\n\n- üìä Generating final predictions\n\n### The end goal is to achieve a competitive relative error in  prediction of the Saleprice of Houses with maximum accuracy.","metadata":{}},{"cell_type":"markdown","source":"We begin by importing essential libraries for data handling, visualization, and modeling.\nThis includes `Pandas, NumPy, Seaborn, Matplotlib`, and multiple machine learning libraries like `Scikit-learn`, `XGBoost`, `LightGBM`, and `CatBoost`.","metadata":{}},{"cell_type":"code","source":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data handling\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Modeling\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\n\n# Display settings\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Load data\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)\ntrain.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Key **observations** after loading the datasets:\n\n- Train set has 1460 rows and 81 columns\n\n- Test set has 1459 rows and 80 columns\n\n- SalePrice is our target variable, available only in the training set.","metadata":{}},{"cell_type":"markdown","source":"# Initial Data Exploration\n\nWe examine the dataset to understand its structure and identify missing values.\n\n- Many categorical columns like `PoolQC`, `Fence`, `Alley`, `MiscFeature` have large sections of missing data.\n\n- Numeric features like `LotFrontage`, `GarageYrBlt`, and `MasVnrArea` have moderate missing values.","metadata":{}},{"cell_type":"code","source":"# Overview of training data\nprint(\"----- TRAIN DATA INFO -----\")\ntrain.info()\n\nprint(\"\\n----- TEST DATA INFO -----\")\ntest.info()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count missing values per column\nmissing = train.isnull().sum()\nmissing = missing[missing > 0].sort_values(ascending=False)\n\nprint(f\"Total columns with missing values: {len(missing)}\")\nmissing.head(20)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### We will summarize numerical statistics using `.describe()` to check data spread, means, and possible outliers.","metadata":{}},{"cell_type":"code","source":"train.describe().T.head(15)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Understanding the Target Variable `(Saleprice)`:\n**Insights:**\n- The price distribution is right-skewed ‚Äî most houses are moderately priced with a few expensive outliers.\n\n- 75% of all houses cost below ~214,000 USD.\n\n- Mean > Median confirms a right-skewed distribution.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.histplot(train['SalePrice'],kde=True)\nplt.title('Distribution of saleprice of Houses')\nplt.show()\nprint(train['SalePrice'].describe())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr = train.select_dtypes(include=[np.number]).corr()['SalePrice'].sort_values(ascending=False)\ncorr.head(15)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above step also helps us realize why a **log transformation** will later improve model stability and reduce skewness.","metadata":{}},{"cell_type":"markdown","source":"# Visual Exploratory Data Analysis (EDA) for (Numeric features)\nWe compute correlations between numeric features and SalePrice.\nTop correlated features:\n\n- `OverallQual`\n\n- `GrLivArea`\n\n- `GarageCars`\n\n- `TotalBsmtSF`\n\n- `1stFlrSF`\n\n  \nLet's visualize how the top correlated numeric features relate to **SalePrice**.\n","metadata":{}},{"cell_type":"code","source":"top_features = ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea','TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']\n\nplt.figure(figsize=(16, 10))\nfor i, feature in enumerate(top_features[:6]):\n    plt.subplot(2, 3, i + 1)\n    sns.scatterplot(data=train, x=feature, y='SalePrice', alpha=0.7)\n    plt.title(f'{feature} vs SalePrice')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Above, we can already see that:\n- Higher **OverallQual** and **GrLivArea** strongly increase SalePrice.\n- There are potential **outliers** ‚Äî very large houses sold at lower prices.\n","metadata":{}},{"cell_type":"markdown","source":"# Outlier Handling & Categorical EDA\n\n## Removing outliers:","metadata":{}},{"cell_type":"code","source":"# Identify potential outliers\noutliers = train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)]\ndisplay(outliers[['Id', 'GrLivArea', 'SalePrice']])\n\n# Remove them from the training set\ntrain = train.drop(outliers.index)\n\nprint(f\"New train shape after removing outliers: {train.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We detected **extreme outliers**: houses with GrLivArea > 4000 but low SalePrice < 300000.\n\nThese can harm the model and introduce bias, so we remove them to improve model generalization.","metadata":{}},{"cell_type":"markdown","source":"## Exploring categorical data:\n\nWe will visualize relationships between categorical variables and SalePrice using **Boxplots**:","metadata":{}},{"cell_type":"code","source":"# Plot settings for exploring Categorical features\nplt.figure(figsize=(14,6))\nsns.boxplot(x='Neighborhood', y='SalePrice', data=train)\nplt.xticks(rotation=45)\nplt.title('SalePrice distribution across Neighborhoods')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights:**\n\nAfter reviewing the above boxplot it can be seen that:\n\n- `Neighborhood`: Some neighborhoods consistently have higher prices.\n","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14,6))\n\nsns.boxplot(x='HouseStyle', y='SalePrice', data=train, ax=axes[0])\naxes[0].set_title('HouseStyle vs SalePrice')\naxes[0].tick_params(axis='x', rotation=45)\n\nsns.boxplot(x='SaleCondition', y='SalePrice', data=train, ax=axes[1])\naxes[1].set_title('SaleCondition vs SalePrice')\naxes[1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Insights üß†**  \n- `HouseStyle`: 2Story and 1Story homes tend to have higher prices, while 1.5Fin and 1.5Unf styles are cheaper.  \n- `SaleCondition`: 'Partial' (new construction) sales are the most expensive.\n\n  \nThese patterns confirm **categorical features are strong predictors**.\n","metadata":{}},{"cell_type":"markdown","source":"# Feaure Engineering and Data Prep\n\n### Handling Missing Values and Imputation:\n\n- Columns where NaN means ‚ÄúNone‚Äù (e.g., `Alley`, `Fence`, `PoolQC`) are filled with \"`None`\".\n\n- Numeric basement/garage values are replaced with `0`.\n\n- LotFrontage is filled using the **median** per Neighborhood.\n\n- GarageYrBlt missing ‚Üí `0`.\n\n- Electrical missing ‚Üí value from **mode**.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\n\n# Workin on a copy\ndf_train = train.copy()\n\n# 1) Columns where NaN means \"None\"\nnone_cols = [\n    'Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n    'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond',\n    'PoolQC','Fence','MiscFeature','MasVnrType'\n]\nfor c in none_cols:\n    if c in df_train.columns:\n        df_train[c] = df_train[c].fillna('None')\n\n# Basement/garage \nzero_cols = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF',\n             'BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea','MasVnrArea']\nfor c in zero_cols:\n    if c in df_train.columns:\n        df_train[c] = df_train[c].fillna(0)\n\n# LotFrontage: impute by Neighborhood median\nif 'LotFrontage' in df_train.columns and 'Neighborhood' in df_train.columns:\n    df_train['LotFrontage'] = df_train.groupby('Neighborhood')['LotFrontage'].transform(\n        lambda s: s.fillna(s.median())\n    )\n\n# GarageYrBlt: missing means no garage\nif 'GarageYrBlt' in df_train.columns:\n    df_train['GarageYrBlt'] = df_train['GarageYrBlt'].fillna(0)\n\n# Electrical: single missing\nif 'Electrical' in df_train.columns:\n    df_train['Electrical'] = df_train['Electrical'].fillna(df_train['Electrical'].mode()[0])\n\n# sanity check of remaining NA\nna_left = df_train.isna().sum()\nna_left = na_left[na_left>0].sort_values(ascending=False)\nprint(\"Still missing after rules:\")\nprint(na_left)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Above output shows that there are **no missing values** anymore.","metadata":{}},{"cell_type":"markdown","source":"## Log-Transforming the target variable `SalePrice`:\n\nWe apply log transformation to SalePrice using np.log1p() bcause,\n\n- it reduces the influence of extremely high-priced houses.\n- Makes data more normally distributed.\n- Helps linear models handle target variability better.","metadata":{}},{"cell_type":"code","source":"# Create a new target variable (log-transformed SalePrice)\ny = np.log1p(train['SalePrice'])   # log(1 + SalePrice)\n\n# Drop SalePrice and Id to get features\nX = train.drop(columns=['SalePrice', 'Id'])\n\n# Check transformation visually\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8,5))\nsns.histplot(y, kde=True, color='skyblue')\nplt.title('Distribution of Log-Transformed SalePrice')\nplt.show()\n\nprint(\"y mean:\", y.mean(), \"y std:\", y.std())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now the distribution of the target `SalePrice` above is not right-skewed anymore when it is log-tansformed.","metadata":{}},{"cell_type":"markdown","source":"## Encoding the categorical values:\n\nWe will separate numeric and categorical columns:\n\n- Apply ordinal mappings to quality-related features (e.g., ExterQual, KitchenQual, BsmtQual, etc.).\n\n- Remaining categorical features are one-hot encoded.\n\nThis encoding allows both linear and non linear models to process mixed data types correctly.","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Create a copy for safety\nX = train.drop(columns=['SalePrice', 'Id']).copy()\n\n# Ordinal mappings (keep meaningful order)\nqual_map = {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}\nbsmt_exp_map = {'None':0,'No':0,'Mn':1,'Av':2,'Gd':3}\nbsmt_fin_map = {'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6}\npaved_map = {'N':0,'P':1,'Y':2}\nbin_map = {'N':0,'Y':1}\nfunctional_map = {'Sal':0,'Sev':1,'Maj2':2,'Maj1':3,'Mod':4,'Min2':5,'Min1':6,'Typ':7}\n\nordinal_maps = {\n    'ExterQual': qual_map, 'ExterCond': qual_map,\n    'BsmtQual': qual_map, 'BsmtCond': qual_map, 'BsmtExposure': bsmt_exp_map,\n    'BsmtFinType1': bsmt_fin_map, 'BsmtFinType2': bsmt_fin_map,\n    'HeatingQC': qual_map, 'KitchenQual': qual_map,\n    'FireplaceQu': qual_map, 'GarageQual': qual_map, 'GarageCond': qual_map,\n    'PoolQC': qual_map, 'PavedDrive': paved_map, 'CentralAir': bin_map,\n    'Functional': functional_map\n}\n\nfor col, mapping in ordinal_maps.items():\n    if col in X.columns:\n        X[col] = X[col].map(mapping).astype('float64')\n\n# Split remaining columns by dtype\nnum_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncat_cols = X.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"Numeric columns: {len(num_cols)} | Categorical columns: {len(cat_cols)}\")\n\n# Preprocess: imputers + one-hot encoder\nnumeric_proc = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median'))\n])\n\ncategorical_proc = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('num', numeric_proc, num_cols),\n        ('cat', categorical_proc, cat_cols)\n    ]\n)\n\nprint(\"Encoding setup complete ‚Äî ready for model training.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Linear Model Training\n## Baseline model training using Ridge Regression:","metadata":{}},{"cell_type":"code","source":"# Baseline Ridge Regression with 10-Fold Cross-Validation\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\n# Create a Ridge regression model inside a pipeline\nridge = Pipeline(steps=[\n    ('preprocessor', preprocess),\n    ('model', Ridge(alpha=10.0, random_state=42))\n])\n\n# Define 10-fold cross-validation setup\ncv = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# Evaluate using Root Mean Squared Log Error\nscores = cross_val_score(\n    ridge, X, y,\n    scoring='neg_root_mean_squared_error',\n    cv=cv,\n    n_jobs=-1\n)\n\nprint(f\"Ridge 10-Fold CV log-RMSE: {-scores.mean():.4f} ¬± {scores.std():.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights:**\n\n- CV log-RMSE: 0.1374 ¬± 0.0379\n\n- This means we have now achieved a **`13.7%` relative prediction error**","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter tuning\nNow that we have the baseline model ready, we can test multiple alpha (regularization) values using `RidgeCV` and `LassoCV`","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import RidgeCV, LassoCV\n\n# Define range of alpha values to test\nalphas = np.logspace(-3, 3, 50)  # from 0.001 to 1000\n\n# RidgeCV\nridge_cv = RidgeCV(alphas=alphas, scoring='neg_root_mean_squared_error', cv=10)\nridge_cv.fit(preprocess.fit_transform(X), y)\nprint(f\"Best Ridge alpha: {ridge_cv.alpha_:.4f}\")\n\n#LassoCV for comparison\nlasso_cv = LassoCV(alphas=alphas, cv=10, random_state=42, max_iter=10000)\nlasso_cv.fit(preprocess.fit_transform(X), y)\nprint(f\"Best Lasso alpha: {lasso_cv.alpha_:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights:**\n- Ridge Œ± ‚âà `10.9854`\n- Lasso Œ± ‚âà `0.0010`\n\nAlso note that the std deviation is still `0.0379`, we will also try to improve this.","metadata":{}},{"cell_type":"markdown","source":"## Rechecking Cross-Validation:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\nbest_alpha_ridge = 10.9854\nbest_alpha_lasso = 0.0010\n\nridge_best = Pipeline([\n    ('preprocessor', preprocess),\n    ('model', Ridge(alpha=best_alpha_ridge, random_state=42))\n])\n\nlasso_best = Pipeline([\n    ('preprocessor', preprocess),\n    ('model', Lasso(alpha=best_alpha_lasso, random_state=42, max_iter=20000))\n])\n\ncv = KFold(n_splits=10, shuffle=True, random_state=42)\n\nridge_scores = cross_val_score(ridge_best, X, y,\n                               scoring='neg_root_mean_squared_error',\n                               cv=cv, n_jobs=-1)\nlasso_scores = cross_val_score(lasso_best, X, y,\n                               scoring='neg_root_mean_squared_error',\n                               cv=cv, n_jobs=-1)\n\nprint(f\"Ridge (Œ±={best_alpha_ridge:.4f})  CV: {-ridge_scores.mean():.4f} ¬± {ridge_scores.std():.4f}\")\nprint(f\"Lasso (Œ±={best_alpha_lasso:.4f})  CV: {-lasso_scores.mean():.4f} ¬± {lasso_scores.std():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights:**\n\n- Ridge CV log-RMSE: `0.1374`\n\n- Lasso CV log-RMSE: `0.1375`\n\nHence, it can be noted that both perform similary when checked with cross validation.","metadata":{}},{"cell_type":"markdown","source":"## Fine-tuning lasso again \n- to check if the true best alpha(Œ±) for Lasso is even smaller `(<0.001)`\n\nFor this, we will test with smaller alpha values raging from `0.00001 to 0.01`","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LassoCV\nimport numpy as np\n\n# Transform once for speed\nX_mm = preprocess.fit_transform(X)\n\nalphas_fine = np.logspace(-5, -2, 30)  # 1e-5 ... 1e-2\nlasso_cv_fine = LassoCV(alphas=alphas_fine, cv=10, random_state=42, max_iter=30000)\nlasso_cv_fine.fit(X_mm, y)\n\nbest_alpha_lasso_refined = float(lasso_cv_fine.alpha_)\nprint(\"Refined best Lasso alpha:\", best_alpha_lasso_refined)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Because the refined Lasso printed an alpha <0.001, we will re-evaluate Lasso with the new alpha `0.0005736152510448681`","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import Lasso\nfrom sklearn.pipeline import Pipeline\n\nlasso_best = Pipeline([\n    ('preprocessor', preprocess),\n    ('model', Lasso(alpha=best_alpha_lasso_refined, random_state=42, max_iter=30000))\n])\n\ncv = KFold(n_splits=10, shuffle=True, random_state=42)\nlasso_scores = cross_val_score(lasso_best, X, y,\n                               scoring='neg_root_mean_squared_error',\n                               cv=cv, n_jobs=-1)\nprint(f\"Lasso (Œ±={best_alpha_lasso_refined:.6f}) CV: {-lasso_scores.mean():.4f} ¬± {lasso_scores.std():.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- So with the updated alpha `Œ±=0.000574` that is far lesser than the older one, it confirms that a smaller **Œ±** helped the Lasso model to learn more detail without overfitting\n- We also improved the relative error in prediction to `0.1342` **(approx. 13.4%)** which was earlier `0.1375`\n\n**Hence, we can say that this is our best performing linear model.**","metadata":{}},{"cell_type":"markdown","source":"# Training and Evaluating Non Linear models \nNow, We will  train the non linear models **`LightGBM`** and **`XGBoost`** with early stopping and CV and then generate OOF (out-of-fold) predictions for both models.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Reuse preprocessed features\nX_mm = preprocess.fit_transform(X)\n\ndef rmse_log(y_true, y_pred):\n    return np.sqrt(np.mean((y_true - y_pred)**2))\n\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# ----- LightGBM -----\nimport lightgbm as lgb\nlgb_params = dict(\n    n_estimators=5000,\n    learning_rate=0.01,\n    num_leaves=31,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n    random_state=42\n)\n\noof_lgb = np.zeros(len(X))\nlgb_best_iters = []\n\nfor tr, va in kf.split(X_mm):\n    X_tr, X_va = X_mm[tr], X_mm[va]\n    y_tr, y_va = y.iloc[tr], y.iloc[va]\n\n    model_lgb = lgb.LGBMRegressor(**lgb_params)\n    model_lgb.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric='rmse',\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n    )\n    oof_lgb[va] = model_lgb.predict(X_va, num_iteration=model_lgb.best_iteration_)\n    lgb_best_iters.append(model_lgb.best_iteration_)\n\nrmse_lgb = rmse_log(y, oof_lgb)\nprint(f\"LightGBM 10-fold OOF log-RMSE: {rmse_lgb:.4f} (avg best_iter ‚âà {int(np.mean(lgb_best_iters))})\")\n\n# ----- XGBoost -----\nfrom xgboost import XGBRegressor\nxgb_params = dict(\n    n_estimators=6000,\n    learning_rate=0.01,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.0,\n    reg_lambda=1.0,\n    objective='reg:squarederror',\n    random_state=42,\n    n_jobs=-1\n)\n\noof_xgb = np.zeros(len(X))\nxgb_best_iters = []\n\nfor tr, va in kf.split(X_mm):\n    X_tr, X_va = X_mm[tr], X_mm[va]\n    y_tr, y_va = y.iloc[tr], y.iloc[va]\n\n    model_xgb = XGBRegressor(**xgb_params)\n    model_xgb.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric='rmse',\n        verbose=False,\n        early_stopping_rounds=200\n    )\n    best_iter = model_xgb.best_iteration if model_xgb.best_iteration is not None else xgb_params['n_estimators']\n    oof_xgb[va] = model_xgb.predict(X_va, iteration_range=(0, best_iter))\n    xgb_best_iters.append(best_iter)\n\nrmse_xgb = rmse_log(y, oof_xgb)\nprint(f\"XGBoost 10-fold OOF log-RMSE: {rmse_xgb:.4f} (avg best_iter ‚âà {int(np.mean(xgb_best_iters))})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights:**\n\nNow we have the OOf predictions for **LightGBM and XGBoost** which is,\n- log RMSE: `0.1247` **approximately 12.47%** for LightGBM\n- and log RMSE: `0.1241` **approximately 12.41%** for XGBoost\n\nNext step is to combine all four of the models' OOF predictions and blend them together for getting the best possible **Out-OF-Fold (OOF) log RMSE**.","metadata":{}},{"cell_type":"markdown","source":"# Generating OOF Predictions for Ridge and Lasso\nWe have to generate the OOF predictions for our Linear models because we already have the OOF log RMSE for the non linear models (LightGBM and XGBoost)\n- we are doing this to match the prediction metrics of all 4 models, so that later we can ensemble them together and get the final OOF log RMSE.\n- This will ensure all four models `(Ridge, Lasso, LightGBM, XGBoost)` are evaluated using consistent folds.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.pipeline import Pipeline\n\n# 10-Fold Cross Validation setup (same folds for all models for fairness)\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# Create Ridge and Lasso pipelines with tuned alphas\nridge_best = Pipeline([\n    ('preprocessor', preprocess),\n    ('model', Ridge(alpha=10.9854, random_state=42))\n])\n\nlasso_best = Pipeline([\n    ('preprocessor', preprocess),\n    ('model', Lasso(alpha=0.0005736152510448681, random_state=42, max_iter=30000))\n])\n\n# Empty arrays to store out-of-fold predictions (same length as training data)\noof_ridge = np.zeros(len(X))\noof_lasso = np.zeros(len(X))\n\n# Generate OOF predictions for Ridge and Lasso\nfor tr, va in kf.split(X):\n    ridge_best.fit(X.iloc[tr], y.iloc[tr])\n    oof_ridge[va] = ridge_best.predict(X.iloc[va])\n\n    lasso_best.fit(X.iloc[tr], y.iloc[tr])\n    oof_lasso[va] = lasso_best.predict(X.iloc[va])\n\n# RMSE in log space (‚âà RMSLE on original scale)\ndef rmse_log(y_true, y_pred):\n    return np.sqrt(np.mean((y_true - y_pred)**2))\n\nridge_rmse = rmse_log(y, oof_ridge) \nlasso_rmse = rmse_log(y, oof_lasso) \nprint(\"Ridge OOF log RMSE\",ridge_rmse), \nprint(\"Lasso OOF log RMSE\",lasso_rmse)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we have the values of **Ridge and Lasso** OOF log RMSE above at approximately `14.25%` Ridge and `14.11%` Lasso, \n- We can now combine all four models and blend them to see if the final log RMSE delivers a lower score than all of the models individually.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Blending All Models \n \n- we will test the best set of weights for the four models by assigning multiple grids of weights for each model.\n- we will only choose the set of weights which proves to deliver the **lowest RMSLE** of all.\n\nNote that we will be **using the OOF prediction values for all 4 models**\n","metadata":{}},{"cell_type":"code","source":"# Candidate weights for Ridge, Lasso, LightGBM, XGBoost\ncandidates = [\n    (0.20, 0.20, 0.30, 0.30),\n    (0.15, 0.15, 0.35, 0.35),\n    (0.10, 0.10, 0.40, 0.40),\n    (0.25, 0.25, 0.25, 0.25),\n    (0.33, 0.33, 0.17, 0.17),\n    (0.10, 0.20, 0.35, 0.35),\n    (0.20, 0.10, 0.35, 0.35),\n]\n\nbest_w = None\nbest_score = 999\n\n# Test each weight combination and pick the best one (lowest RMSLE)\nfor w in candidates:\n    w_r, w_l, w_lb, w_x = w\n    blend = w_r*oof_ridge + w_l*oof_lasso + w_lb*oof_lgb + w_x*oof_xgb\n    score = rmse_log(y, blend)\n    print(f\"Weights {w} ‚Üí OOF log-RMSE: {score:.4f}\")\n    if score < best_score:\n        best_score, best_w = score, w\n\nprint(f\"\\n Best OOF blend: weights={best_w}, score={best_score:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now after reviewing the final log RMSE scores with respect to each grid of weights assigned after combining all 4 models,\n- It can be said that `0.1206` is the best score until now.\n- It should also be noted that `0.1206` **(approx.12%)** is a better score than all of the previously predicted OOF log RMSE of each model individually.\n\nWe can now move ahead and train our best grid of weights, `weights=(0.1, 0.1, 0.4, 0.4)` on full data.","metadata":{}},{"cell_type":"markdown","source":"# Training and Testing on entire data\n\n### Handling a Preprocessing Error Before Final Training:\n\nWhile preparing the final models for full training and test prediction,  \nI encountered a **ValueError** related to missing value imputation:\n\n> `ValueError: Cannot use median strategy with non-numeric data: could not convert string to float: 'TA'`\n\nThis occurred because some **ordinal categorical features** (like `ExterQual`, `BsmtQual`, `KitchenQual`, etc.)  \ncontained string values (`'TA'`, `'Gd'`, `'Ex'`, etc.) in the **test set**,  \nwhich conflicted with the **numeric median imputer** in the preprocessing pipeline.\n\n## Step A: Ordinal Mapping & Combined Preprocessing","metadata":{}},{"cell_type":"code","source":"# MAP ORDINALS & FIT PREPROCESSOR ON TRAIN + TEST\n\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Ridge, Lasso\n\n# Use average best iterations from early stopping\navg_lgb_iter = int(np.mean(lgb_best_iters))\navg_xgb_iter = int(np.mean(xgb_best_iters))\n\n# making copies\nX_train_fixed = X.copy()\nX_test_fixed  = test.drop(columns=['Id']).copy()\n\n# Define the same ordinal maps used earlier\nqual_map       = {'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}\nbsmt_exp_map   = {'None':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4}\nbsmt_fin_map   = {'None':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}\npaved_map      = {'N':0, 'P':1, 'Y':2}\nbin_map        = {'N':0, 'Y':1}\nfunctional_map = {'Sal':1,'Sev':2,'Maj2':3,'Maj1':4,'Mod':5,'Min2':6,'Min1':7,'Typ':8}\n\nordinal_maps = {\n    'ExterQual': qual_map, 'ExterCond': qual_map,\n    'BsmtQual': qual_map, 'BsmtCond': qual_map, 'BsmtExposure': bsmt_exp_map,\n    'BsmtFinType1': bsmt_fin_map, 'BsmtFinType2': bsmt_fin_map,\n    'HeatingQC': qual_map, 'KitchenQual': qual_map,\n    'FireplaceQu': qual_map, 'GarageQual': qual_map, 'GarageCond': qual_map,\n    'PoolQC': qual_map, 'PavedDrive': paved_map, 'CentralAir': bin_map,\n    'Functional': functional_map\n}\n\ndef apply_ordinal_maps_inplace(df):\n    for col, mp in ordinal_maps.items():\n        if col in df.columns:\n            if df[col].dtype == 'O' or df[col].dtype.name == 'category':\n                df[col] = df[col].fillna('None').map(mp)\n            df[col] = df[col].astype('float', errors='ignore')\n\napply_ordinal_maps_inplace(X_train_fixed)\napply_ordinal_maps_inplace(X_test_fixed)\n\n# Fit existing preprocessor on TRAIN + TEST combined\ncombined = pd.concat([X_train_fixed, X_test_fixed], axis=0)\npreprocess_full = preprocess.fit(combined)\n\n# Transform train and test with fitted preprocessor\nX_full    = preprocess_full.transform(X_train_fixed)\nX_test_mm = preprocess_full.transform(X_test_fixed)\n\nprint(\"‚úÖ Preprocessing completed. Shapes:\", X_full.shape, X_test_mm.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Insights:\n\n**The fix for above error:**\n- Mapped ordinal strings (like `TA`, `Gd`, `Ex`) to numbers in both train and test sets.\n\n- Refit the preprocessor on combined train + test data to learn all category levels.\n\n- Transformed both datasets again for clean modeling.","metadata":{}},{"cell_type":"markdown","source":"## Step B: Train all models on full data\n\nWe train all four models on the full dataset using tuned parameters:\n\n- **Ridge** (Œ± = 10.9854)\n\n- **Lasso** (Œ± = 0.000574)\n\n- **LightGBM** (best_iter ‚âà 1879)\n\n- **XGBoost** (best_iter ‚âà 2251)","metadata":{}},{"cell_type":"code","source":"# ==============================\n# üß© PART 3B ‚Äî TRAIN FULL MODELS (LIGHTGBM, XGBOOST, RIDGE, LASSO)\n# ==============================\n\n# Train LightGBM on full data\nlgb_params = dict(\n    n_estimators=avg_lgb_iter,\n    learning_rate=0.01,\n    num_leaves=31,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n    random_state=42\n)\nlgb_full = lgb.LGBMRegressor(**lgb_params)\nlgb_full.fit(X_full, y)\n\n# Train XGBoost on full data\nxgb_params = dict(\n    n_estimators=avg_xgb_iter,\n    learning_rate=0.01,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.0,\n    reg_lambda=1.0,\n    objective='reg:squarederror',\n    random_state=42,\n    n_jobs=-1\n)\nxgb_full = XGBRegressor(**xgb_params)\nxgb_full.fit(X_full, y, verbose=False)\n\n# Train Ridge and Lasso on full data\nridge_full = Ridge(alpha=10.9854, random_state=42)\nridge_full.fit(X_full, y)\n\nlasso_full = Lasso(alpha=0.0005736152510448681, random_state=42, max_iter=30000)\nlasso_full.fit(X_full, y)\n\nprint(\"Models trained on full data (Ridge, Lasso, LightGBM, XGBoost).\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### All models are now blended and trained together successfully.","metadata":{}},{"cell_type":"markdown","source":"## Step C: Predicting Test set, Blending final Results and Creating submission file.\n\nIn this final step, We will generate predictions for the test set:\n\n- Get log predictions from each model.\n\n- Blend them using the best weights `(0.1, 0.1, 0.4, 0.4)`.\n\n- Convert log predictions back to actual Dollar prices.","metadata":{}},{"cell_type":"code","source":"# Get log predictions from each model\npred_ridge_log = ridge_full.predict(X_test_mm)\npred_lasso_log = lasso_full.predict(X_test_mm)\npred_lgb_log   = lgb_full.predict(X_test_mm)\npred_xgb_log   = xgb_full.predict(X_test_mm)\n\n# Blend predictions using best weights\nw_r, w_l, w_lb, w_x = best_w  # Example: (0.1, 0.1, 0.4, 0.4)\npred_log = (w_r*pred_ridge_log +\n            w_l*pred_lasso_log +\n            w_lb*pred_lgb_log +\n            w_x*pred_xgb_log)\n\n# Convert log predictions back to actual dollar prices\npred = np.expm1(pred_log)\npred = np.clip(pred, 0, None)\n\n# Create final submission file\nsub = test[['Id']].copy()\nsub['SalePrice'] = pred\nsub.to_csv('submission.csv', index=False)\n\nprint(f\"\\nSaved submission.csv with weights={best_w}, OOF blend score={best_score:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Above we finally recieved a competitive OOF blend score of all 4 models, that is: `0.1206` **(approx. 12% error)** \n\n# üß† Key Learnings:\n- Regularization improves linear stability (Ridge, Lasso).\n\n- Tree-based models handle complex non-linear relationships better.\n\n- Blending reduces overfitting and improves generalization.\n\n- Proper preprocessing and ordinal mapping are crucial for error-free model pipelines.\n","metadata":{}},{"cell_type":"markdown","source":"# UPDATE - Trying to achieve an even better relative error than `12%`","metadata":{}},{"cell_type":"markdown","source":"# Part A ‚Äî Feature engineering + neighborhood target encoding + preprocess\n\n## Part A.1","metadata":{}},{"cell_type":"code","source":"# FEATURE ENGINEERING (applied to BOTH train & test)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Start from your existing frames\nX_base   = X.copy()\ntest_base = test.copy()\n\ndef add_features(df):\n    df = df.copy()\n\n    # 1) Total square footage (finished areas)\n    df['TotalSF'] = df.get('TotalBsmtSF', 0) + df.get('1stFlrSF', 0) + df.get('2ndFlrSF', 0)\n\n    # 2) Total bathrooms (full + half baths upstairs + basement)\n    df['TotalBath'] = (\n        df.get('FullBath', 0)\n        + 0.5 * df.get('HalfBath', 0)\n        + df.get('BsmtFullBath', 0)\n        + 0.5 * df.get('BsmtHalfBath', 0)\n    )\n\n    # 3) Ages relative to sale year (how old things are)\n    df['Age']       = (df.get('YrSold', 0) - df.get('YearBuilt', 0)).clip(lower=0)\n    df['RemodAge']  = (df.get('YrSold', 0) - df.get('YearRemodAdd', 0)).clip(lower=0)\n    df['GarageAge'] = (df.get('YrSold', 0) - df.get('GarageYrBlt', 0)).clip(lower=0)\n\n    # 4) Porch/Deck footprint and a binary flag\n    porch_cols = ['WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch']\n    df['PorchSF']  = df[porch_cols].sum(axis=1)\n    df['HasPorch'] = (df['PorchSF'] > 0).astype(int)\n\n    # 5) Interaction: quality √ó living area (bigger *and* better quality costs more)\n    df['Qual_x_GrLiv'] = df.get('OverallQual', 0) * df.get('GrLivArea', 0)\n\n    # 6) Log transforms for very skewed size features (keep originals too)\n    for c in ['GrLivArea', 'TotalSF', 'LotArea']:\n        if c in df.columns:\n            df[f'log1p_{c}'] = np.log1p(df[c])\n\n    # 7) New-build hint from SaleCondition\n    df['IsNew'] = (df.get('SaleCondition', 'Normal') == 'Partial').astype(int)\n\n    return df\n\nX_fe    = add_features(X_base)\ntest_fe = add_features(test_base)\nprint(\"Added engineered features. Example new cols:\",\n      [c for c in ['TotalSF','TotalBath','Age','PorchSF','HasPorch','Qual_x_GrLiv','log1p_GrLivArea'] if c in X_fe.columns][:7])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights:**\n\n- `TotalSF`: because buyers pay for total finished area, not just one floor.\n\n- `TotalBath`: more baths = higher price; half baths count as 0.5.\n\n- `Age` / `RemodAge` / `GarageAge`: newer houses/remodels/garages usually sell higher.\n\n- `PorchSF`/`HasPorch`: outdoor space adds value.\n\n- `Qual√óGrLiv`: large houses with high quality get a price ‚Äúboost‚Äù beyond a simple sum.\n\n- `log1p(size)`: normalizes extreme values so models learn smoother relations.\n\n- `IsNew`: SaleCondition='Partial' often means new construction.","metadata":{}},{"cell_type":"markdown","source":"## Part A.2 - Target Encoding for Neighborhood (Leak-Free)\nIn this step, I created a new feature called `TE_Neighborhood`, which assigns each neighborhood the average log sale price based only on training folds.\nI used leak-free K-Fold target encoding, meaning each row‚Äôs encoded value is computed from other folds and never from itself.\n","metadata":{}},{"cell_type":"code","source":"# LEAK-FREE TARGET ENCODING FOR Neighborhood\n\nfrom sklearn.model_selection import KFold\n\n# y is your log-transformed target from earlier steps (np.log1p(SalePrice))\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nglobal_mean = y.mean()  # fallback\n\n# Out-of-fold means for train\nte_tr = pd.Series(index=X_fe.index, dtype=float)\n\nfor tr_idx, val_idx in kf.split(X_fe):\n    tr_neigh  = X_fe.loc[tr_idx, 'Neighborhood'].astype(str)\n    val_neigh = X_fe.loc[val_idx, 'Neighborhood'].astype(str)\n    means = y.iloc[tr_idx].groupby(tr_neigh).mean()  # mean log(SalePrice) per neighborhood in the TRAIN fold only\n    te_tr.iloc[val_idx] = val_neigh.map(means).fillna(global_mean)\n\nX_fe['TE_Neighborhood'] = te_tr\n\n# Apply train means to test\ntrain_means_final = y.groupby(X_fe['Neighborhood'].astype(str)).mean()\ntest_fe['TE_Neighborhood'] = (\n    test_fe['Neighborhood'].astype(str).map(train_means_final).fillna(global_mean)\n)\n\nprint(\"Target-encoded Neighborhood added as TE_Neighborhood.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights:**\n\n- Encoding is done using **10-fold CV**\n\n- Training data = out-of-fold neighborhood means\n\n- Test data = neighborhood means computed from full train\n\n- Global mean is used for unseen neighborhood levels\n","metadata":{}},{"cell_type":"markdown","source":"## Part A.3 ‚Äî Rebuilding Preprocessing After Feature Engineering\n\nSince feature engineering added new numerical features (like `TotalSF`,`Age`, log features, TE features, etc.), I had to refit the entire preprocessing pipeline on both train + test rows combined.","metadata":{}},{"cell_type":"code","source":"# APPLY ORDINAL MAPS + PREPROCESS + TRANSFORM\n\n# Reusing ordinal quality maps\nqual_map       = {'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}\nbsmt_exp_map   = {'None':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4}\nbsmt_fin_map   = {'None':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}\npaved_map      = {'N':0, 'P':1, 'Y':2}\nbin_map        = {'N':0, 'Y':1}\nfunctional_map = {'Sal':1,'Sev':2,'Maj2':3,'Maj1':4,'Mod':5,'Min2':6,'Min1':7,'Typ':8}\n\nordinal_maps = {\n    'ExterQual': qual_map, 'ExterCond': qual_map,\n    'BsmtQual': qual_map, 'BsmtCond': qual_map, 'BsmtExposure': bsmt_exp_map,\n    'BsmtFinType1': bsmt_fin_map, 'BsmtFinType2': bsmt_fin_map,\n    'HeatingQC': qual_map, 'KitchenQual': qual_map,\n    'FireplaceQu': qual_map, 'GarageQual': qual_map, 'GarageCond': qual_map,\n    'PoolQC': qual_map, 'PavedDrive': paved_map, 'CentralAir': bin_map,\n    'Functional': functional_map\n}\n\ndef apply_ordinal_maps_inplace(df):\n    for col, mp in ordinal_maps.items():\n        if col in df.columns:\n            if df[col].dtype == 'O' or df[col].dtype.name == 'category':\n                df[col] = df[col].fillna('None').map(mp)\n            df[col] = df[col].astype('float', errors='ignore')\n\napply_ordinal_maps_inplace(X_fe)\napply_ordinal_maps_inplace(test_fe)\n\n# Build a fresh ColumnTransformer on the engineered frames\nnumeric_features     = X_fe.select_dtypes(include=[np.number]).columns.tolist()\ncategorical_features = X_fe.select_dtypes(exclude=[np.number]).columns.tolist()\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler(with_mean=False))  # safe for sparse outputs downstream\n])\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))\n])\n\npreprocess_v2 = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features),\n    ],\n    remainder='drop'\n)\n\n# Fit on train+test combined features so encoders see all categories\ncombined_v2 = pd.concat([X_fe, test_fe], axis=0)\npreprocess_v2.fit(combined_v2)\n\n# Transform to model-ready matrices\nXmm       = preprocess_v2.transform(X_fe)\nXmm_test  = preprocess_v2.transform(test_fe)\n\nprint(\"Shapes after FE + TE + preprocess:\", Xmm.shape, Xmm_test.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PART B ‚Äî Out-of-Fold Predictions for Base Models\n\n## Part B.1 - Training Base Models to Generate OOF Predictions\nHere I trained five different base models using 10-fold out-of-fold (OOF) training:\n\n- Ridge Regression\n\n- Lasso Regression\n\n- ElasticNet\n\n- LightGBM\n\n- XGBoost\n\nI used the same fold splits for all models so their OOF predictions align row-by-row.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nimport numpy as np\nimport pandas as pd\n\ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nn = Xmm.shape[0]\n\n# storage\noof = {}\ntest_preds = {}\n\ndef fit_oof(name, model_fn, Xtr, ytr, Xte, folds=kf):\n    oof_pred = np.zeros(n)\n    te_pred  = np.zeros(Xte.shape[0])\n    best_iters = []\n\n    for tr_idx, val_idx in folds.split(Xtr):\n        X_tr, X_val = Xtr[tr_idx], Xtr[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n\n        m = model_fn()\n\n        # LightGBM special fitting (early stopping)\n        if isinstance(m, lgb.LGBMRegressor):\n            m.fit(X_tr, y_tr,\n                  eval_set=[(X_val, y_val)],\n                  eval_metric='rmse',\n                  callbacks=[lgb.early_stopping(200, verbose=False)])\n            best_iters.append(m.best_iteration_)\n            oof_pred[val_idx] = m.predict(X_val, num_iteration=m.best_iteration_)\n            te_pred += m.predict(Xte, num_iteration=m.best_iteration_) / folds.get_n_splits()\n\n        # XGBoost special fitting\n        elif isinstance(m, XGBRegressor):\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_val, y_val)],\n                eval_metric='rmse',\n                verbose=False,\n                early_stopping_rounds=200\n            )\n    \n            # In new XGBoost versions:\n            best_iters.append(m.best_iteration)\n\n            # OOF predictions using best_iteration\n            oof_pred[val_idx] = m.predict(X_val, iteration_range=(0, m.best_iteration))\n\n            # Test predictions averaged across folds\n            te_pred += m.predict(Xte, iteration_range=(0, m.best_iteration)) / folds.get_n_splits()\n\n\n        else:\n            # linear models\n            m.fit(X_tr, y_tr)\n            oof_pred[val_idx] = m.predict(X_val)\n            te_pred += m.predict(Xte) / folds.get_n_splits()\n\n    print(f\"{name}: OOF log-RMSE = {rmsle(y, oof_pred):.5f}\", \n          (f\" | avg best_iter ‚âà {int(np.mean(best_iters))}\" if best_iters else \"\"))\n\n    oof[name] = oof_pred\n    test_preds[name] = te_pred\n\n\n# --- Linear models (strong tuning) ---\ndef ridge_model():\n    return Ridge(alpha=8.0, random_state=42)\n\ndef lasso_model():\n    return Lasso(alpha=5.7e-4, max_iter=60000, random_state=42)\n\ndef enet_model():\n    return ElasticNet(alpha=1e-4, l1_ratio=0.2, max_iter=60000, random_state=42)\n\nfit_oof(\"Ridge\", ridge_model, Xmm, y, Xmm_test)\nfit_oof(\"Lasso\", lasso_model, Xmm, y, Xmm_test)\nfit_oof(\"ElasticNet\", enet_model, Xmm, y, Xmm_test)\n\n# --- LightGBM (stronger settings) ---\ndef lgbm_model():\n    return lgb.LGBMRegressor(\n        learning_rate=0.005,\n        n_estimators=12000,\n        num_leaves=31,\n        min_data_in_leaf=10,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=0.5,\n        random_state=42\n    )\n\n# --- XGBoost (stronger settings) ---\ndef xgb_model():\n    return XGBRegressor(\n        learning_rate=0.008,\n        n_estimators=15000,\n        max_depth=4,\n        min_child_weight=1,\n        subsample=0.75,\n        colsample_bytree=0.75,\n        reg_alpha=0.001,\n        reg_lambda=1.0,\n        objective='reg:squarederror',\n        random_state=42,\n        n_jobs=-1\n    )\n\nfit_oof(\"LightGBM\", lgbm_model, Xmm, y, Xmm_test)\nfit_oof(\"XGBoost\",  xgb_model,  Xmm, y, Xmm_test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part B.2 ‚Äî Building the Stacking Matrices","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nstack_train = pd.DataFrame({name: oof[name] for name in oof})\nstack_test  = pd.DataFrame({name: test_preds[name] for name in test_preds})\n\nprint(\"Stack train shape:\", stack_train.shape)\nprint(\"Stack test shape:\", stack_test.shape)\n\nstack_train.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights:**\n\nUsing the OOF predictions, I created two new matrices:\n\n`stack_train` ‚Üí shape (1460, 5)\nContains OOF predictions from all 5 base models for training rows.\n\n`stack_test` ‚Üí shape (1459, 5)\nContains averaged predictions from all 5 base models for test rows.\n\nThese two matrices will be the input to the stacking meta-model in the next part.","metadata":{}},{"cell_type":"markdown","source":"# PART C ‚Äî Final Stacked Meta-Model\n\nIn this final step, I trained a **Lasso meta-model** on top of the stacking matrices.\nThe model learns how to combine Ridge, Lasso, ElasticNet, LightGBM, and XGBoost.\n\nI used 10-fold CV again to generate meta-level OOF predictions and then used the OOf predictions test predictions across folds.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# 10-fold CV for meta-model\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\n\noof_meta = np.zeros(stack_train.shape[0])     # OOF predictions for meta-model\ntest_meta = np.zeros(stack_test.shape[0])     # Test predictions averaged over folds\n\nfor tr_idx, val_idx in kf.split(stack_train):\n    \n    # Training part of stacking features\n    X_tr, X_val = stack_train.iloc[tr_idx], stack_train.iloc[val_idx]\n    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n\n    # Meta-model (Lasso works incredibly well here)\n    meta_model = Lasso(alpha=0.0005, max_iter=50000, random_state=42)\n    meta_model.fit(X_tr, y_tr)\n\n    # OOF prediction for validation fold\n    oof_meta[val_idx] = meta_model.predict(X_val)\n\n    # Prediction for test set (accumulated)\n    test_meta += meta_model.predict(stack_test) / kf.get_n_splits()\n\n# Print meta-model performance\nprint(\"Meta-model OOF log-RMSE:\", rmsle(y, oof_meta))\n\n# Final predictions from stacked model\nfinal_log_preds = test_meta\nfinal_preds = np.expm1(final_log_preds)\nfinal_preds = np.clip(final_preds, 0, None)\n\n# Save submission\nsub = test[['Id']].copy()\nsub['SalePrice'] = final_preds\nsub.to_csv('submission.csv', index=False)\n\nprint(\"\\n‚úÖ Final stacked submission saved as submission.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model accuracy (Interpreting the final score)\n\nMy final stacked meta-model achieved an OOF log-RMSE of **`0.1163`**.  \nHence, this score translates to an approximate **11.6% relative error** in predicting the `SalePrice` of houses.\n\nThis means that, on average, my predictions differ from the actual prices by roughly **¬±11‚Äì12%**, which is considered a strong performance for this competition.\n\n\n**Insights:**\n\n- Log predictions were converted back to actual prices\n\n- Negative values were clipped\n\n- The final CSV **(submission.csv)** was created for Kaggle submission","metadata":{}},{"cell_type":"markdown","source":"## üß† Key Learnings\n\n- Feature engineering (TotalSF, Age, baths, porches, TE) adds strong predictive signal.\n- Leak-free target encoding prevents data leakage and improves model stability.\n- Rebuilding preprocessing after FE keeps train/test feature spaces fully aligned.\n- OOF predictions create clean inputs for stacking without overfitting.\n- Boosting models capture non-linear patterns that linear models miss.\n- Stacking blends strengths of all models and reduces variance.\n- Lasso meta-model learns the best combination of base model predictions.\n\n","metadata":{}}]}